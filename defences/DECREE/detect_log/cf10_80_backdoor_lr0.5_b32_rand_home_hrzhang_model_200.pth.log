Namespace(arch='resnet18', batch_size=32, encoder_path='/home/hrzhang/projects/badencoder_filter/output/cifar10/stl10_backdoored_encoder/2023-12-30-00:04:40/model_200.pth', encoder_usage_info='cifar10', gpu='5', id='_home_hrzhang_model_200.pth', lr=0.5, mask_init='rand', model_flag='backdoor', result_file='resultfinal_filter_txt.txt', seed=80, thres=0.99)
backdoor loaded: /home/hrzhang/projects/badencoder_filter/output/cifar10/stl10_backdoored_encoder/2023-12-30-00:04:40/model_200.pth
trigger: trigger/trigger_pt_white_21_10_ap_replace.npz
mask_size:32
shadow transform: Compose(
    Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
)
shadow dataset size: 1000
Config: lambda_min: 1e-05, adapt_lambda: 5.0, lambda_set_patience: 10,succ_threshold: 0.99, early_stop_patience: 10,
epoch: 0  lr: 0.5000
step5:loss_lambda is down to 0.0002
step11:loss_lambda is down to 4e-05
step17:loss_lambda is down to 8.000000000000001e-06
step23:loss_lambda is down to 8.000000000000001e-06
e=0, loss=-0.786147, loss_cos=-0.961871, loss_reg=623.936950, cur_reg_best=689.251106, es_reg_best:689.251106
epoch: 1  lr: 0.5000
step5:loss_lambda is up to 4.000000000000001e-05
step11:loss_lambda is up to 0.00020000000000000004
step18:loss_lambda is down to 4.000000000000001e-05
e=1, loss=-0.949737, loss_cos=-0.986797, loss_reg=626.510657, cur_reg_best=591.474994, es_reg_best:591.474994
epoch: 2  lr: 0.5000
step0:loss_lambda is up to 0.00020000000000000004
step6:loss_lambda is down to 4.000000000000001e-05
step16:loss_lambda is up to 0.00020000000000000004
step22:loss_lambda is down to 4.000000000000001e-05
step28:loss_lambda is down to 8.000000000000001e-06
e=2, loss=-0.936154, loss_cos=-0.983147, loss_reg=515.623351, cur_reg_best=583.326007, es_reg_best:583.326007
epoch: 3  lr: 0.5000
step5:loss_lambda is up to 4.000000000000001e-05
step11:loss_lambda is up to 0.00020000000000000004
step18:loss_lambda is down to 4.000000000000001e-05
step24:loss_lambda is down to 8.000000000000001e-06
step30:loss_lambda is up to 4.000000000000001e-05
e=3, loss=-0.956325, loss_cos=-0.987308, loss_reg=571.807267, cur_reg_best=543.685733, es_reg_best:543.685733
epoch: 4  lr: 0.5000
step4:loss_lambda is up to 0.00020000000000000004
step11:loss_lambda is down to 4.000000000000001e-05
step17:loss_lambda is down to 8.000000000000001e-06
step23:loss_lambda is up to 4.000000000000001e-05
step29:loss_lambda is up to 0.00020000000000000004
e=4, loss=-0.947738, loss_cos=-0.986897, loss_reg=554.350822, cur_reg_best=465.404481, es_reg_best:465.404481
epoch: 5  lr: 0.5000
step5:loss_lambda is down to 4.000000000000001e-05
step25:loss_lambda is up to 0.00020000000000000004
e=5, loss=-0.940294, loss_cos=-0.984006, loss_reg=489.735121, cur_reg_best=465.404481, es_reg_best:465.404481
epoch: 6  lr: 0.5000
step0:loss_lambda is down to 4.000000000000001e-05
step17:loss_lambda is up to 0.00020000000000000004
step24:loss_lambda is down to 4.000000000000001e-05
step30:loss_lambda is down to 8.000000000000001e-06
e=6, loss=-0.949728, loss_cos=-0.985682, loss_reg=498.015357, cur_reg_best=465.404481, es_reg_best:465.404481
epoch: 7  lr: 0.5000
step4:loss_lambda is up to 4.000000000000001e-05
step10:loss_lambda is up to 0.00020000000000000004
step17:loss_lambda is down to 4.000000000000001e-05
e=7, loss=-0.954638, loss_cos=-0.987493, loss_reg=528.014280, cur_reg_best=465.404481, es_reg_best:465.404481
epoch: 8  lr: 0.5000
step12:loss_lambda is up to 0.00020000000000000004
step18:loss_lambda is down to 4.000000000000001e-05
step30:loss_lambda is up to 0.00020000000000000004
e=8, loss=-0.951738, loss_cos=-0.987701, loss_reg=514.734674, cur_reg_best=465.404481, es_reg_best:465.404481
epoch: 9  lr: 0.5000
step4:loss_lambda is down to 4.000000000000001e-05
step21:loss_lambda is up to 0.00020000000000000004
step28:loss_lambda is down to 4.000000000000001e-05
e=9, loss=-0.939929, loss_cos=-0.983786, loss_reg=485.239101, cur_reg_best=465.404481, es_reg_best:465.404481
epoch: 10  lr: 0.5000
step2:loss_lambda is down to 8.000000000000001e-06
step8:loss_lambda is up to 4.000000000000001e-05
step14:loss_lambda is up to 0.00020000000000000004
step20:loss_lambda is down to 4.000000000000001e-05
step26:loss_lambda is down to 8.000000000000001e-06
e=10, loss=-0.960383, loss_cos=-0.988059, loss_reg=530.400800, cur_reg_best=465.404481, es_reg_best:465.404481
epoch: 11  lr: 0.5000
step5:loss_lambda is up to 4.000000000000001e-05
step11:loss_lambda is up to 0.00020000000000000004
step18:loss_lambda is down to 4.000000000000001e-05
step29:loss_lambda is up to 0.00020000000000000004
e=11, loss=-0.951184, loss_cos=-0.987273, loss_reg=529.936439, cur_reg_best=460.945347, es_reg_best:460.945347
epoch: 12  lr: 0.5000
step5:loss_lambda is down to 4.000000000000001e-05
step18:loss_lambda is up to 0.00020000000000000004
step25:loss_lambda is down to 4.000000000000001e-05
e=12, loss=-0.937673, loss_cos=-0.982023, loss_reg=457.465577, cur_reg_best=460.945347, es_reg_best:460.945347
epoch: 13  lr: 0.5000
step30:loss_lambda is up to 0.00020000000000000004
e=13, loss=-0.967596, loss_cos=-0.990657, loss_reg=513.057077, cur_reg_best=460.945347, es_reg_best:460.945347
Early stop!
End:40.1942s
L1:460.9453:/home/hrzhang/projects/badencoder_filter/output/cifar10/stl10_backdoored_encoder/2023-12-30-00:04:40/model_200.pth:
reg: 623.94,626.51,515.62,571.81,554.35,489.74,498.02,528.01,514.73,485.24,530.4,529.94,457.47,513.06
cos: 0.96,0.99,0.98,0.99,0.99,0.98,0.99,0.99,0.99,0.98,0.99,0.99,0.98,0.99
